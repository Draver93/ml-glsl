#version 430

layout(local_size_x = 32, local_size_y = 1, local_size_z = 1) in;

layout(binding = 0, std430) restrict readonly buffer OutputBuffer { float outputs[]; };
layout(binding = 1, std430) restrict readonly buffer TargetBuffer { float targets[]; };
layout(binding = 2, std430) restrict readonly buffer PreActivationBuffer { float preactivations[]; };

layout(binding = 3, std430) restrict writeonly buffer DeltaBuffer { float deltas[]; };

uniform int output_size;
uniform int batch_size;
uniform int activation_type;

float derivative(float z, int type) {
    switch(type) {
        case 0: { // tanh derivative using pre-activation
            float y = tanh(z);
            return 1.0 - y * y;
        }
        case 1: return z > 0.0 ? 1.0 : 0.0; // relu derivative
        case 2: return z > 0.0 ? 1.0 : 0.01; // leaky relu derivative
        case 3: { // sigmoid derivative using pre-activation
            float y = 1.0 / (1.0 + exp(-z));
            return y * (1.0 - y);
        }
        case 4: return 1.0; // identity derivative
        default: return 1.0;
    }
}

void main() {
    uint batch_idx = gl_GlobalInvocationID.x / output_size;
    uint neuron_idx = gl_GlobalInvocationID.x % output_size;
    
    if (batch_idx >= batch_size || neuron_idx >= output_size) return;
    
    uint idx = batch_idx * output_size + neuron_idx;
    float output_val = outputs[idx];
    float target_val = targets[idx];
    float preactivation_val = preactivations[idx];
    
    // MSE derivative: 2 * (output - target)
    float error = 2.0 * (output_val - target_val);
    
    // Apply activation derivative using pre-activation value
    deltas[idx] = error * derivative(preactivation_val, activation_type);
}