#version 430
layout(local_size_x = 16, local_size_y = 16) in;

layout(std430) readonly buffer BufferQ { float Q[]; };         // [seq_len, model_dim]
layout(std430) readonly buffer BufferK { float K[]; };         // [seq_len, model_dim]
layout(std430) writeonly buffer RawScores { float scores[]; }; // [num_heads * seq_len, seq_len]

uniform int seq_len;
uniform int head_dim;
uniform int num_heads;
uniform int use_mask;
uniform float inv_sqrt_head_dim;

void main() {
    uint i = gl_GlobalInvocationID.x;  // query position
    uint j = gl_GlobalInvocationID.y;  // key position
    
    if (i >= uint(seq_len) || j >= uint(seq_len)) return;
    
    // Apply causal mask if enabled
    if (use_mask == 1 && j > i) {
        // Set -infinity for all heads at this position
        for (int h = 0; h < num_heads; h++) {
            scores[(h * seq_len + int(i)) * seq_len + int(j)] = -3.402823e38;
        }
        return;
    }
    
    // Compute attention scores for each head
    for (int h = 0; h < num_heads; h++) {
        // Compute dot product: Q[i, head_h] @ K[j, head_h]
        float score = 0.0;
        for (int d = 0; d < head_dim; d++) {
            int q_idx = int(i) * (num_heads * head_dim) + h * head_dim + d;
            int k_idx = int(j) * (num_heads * head_dim) + h * head_dim + d;
            score += Q[q_idx] * K[k_idx];
        }
        
        // Scale by 1/sqrt(head_dim)
        score *= inv_sqrt_head_dim;
        
        // Store score for this head
        scores[(h * seq_len + int(i)) * seq_len + int(j)] = score;
    }
}