#version 430
layout(local_size_x = 16) in;

layout(std430) readonly buffer GradAttentionWeights { float grad_attn[]; };  // [seq_len, seq_len]
layout(std430) readonly buffer CachedAttentionWeights { float cached_attn[]; }; // [seq_len, seq_len]
layout(std430) writeonly buffer GradScores { float grad_scores[]; };         // [seq_len, seq_len]

uniform int seq_len;
uniform int use_mask;

void main() {
    uint row = gl_GlobalInvocationID.x;
    if (row >= uint(seq_len)) return;
    
    // Compute sum for softmax gradient
    float sum_grad_attn = 0.0;
    for (int j = 0; j < seq_len; j++) {
        if (use_mask == 1 && j > row) continue;
        sum_grad_attn += grad_attn[row * seq_len + j] * cached_attn[row * seq_len + j];
    }
    
    // Compute gradient w.r.t. scores
    for (int j = 0; j < seq_len; j++) {
        if (use_mask == 1 && j > row) {
            grad_scores[row * seq_len + j] = 0.0;
        } else {
            grad_scores[row * seq_len + j] = cached_attn[row * seq_len + j] * 
                (grad_attn[row * seq_len + j] - sum_grad_attn);
        }
    }
}
