#version 430
layout(local_size_x = 16) in;

layout(std430) readonly buffer GradAttentionWeights { float grad_attn[]; };  // [seq_len, seq_len]
layout(std430) readonly buffer CachedAttentionWeights { float cached_attn[]; }; // [seq_len, seq_len]
layout(std430) writeonly buffer GradScores { float grad_scores[]; };         // [seq_len, seq_len]

uniform int seq_len;
uniform int use_mask;

// Padding mask: 1 for real tokens, 0 for PAD tokens
layout(std430) readonly buffer PaddingMask { int padding_mask[]; };

void main() {
    uint row = gl_GlobalInvocationID.x;
    if (row >= uint(seq_len)) return;
    
    // Compute sum for softmax gradient (excluding masked positions)
    float sum_grad_attn = 0.0;
    for (int j = 0; j < seq_len; j++) {
        if (use_mask == 1 && j > row) continue;  // Apply causal mask
        if (padding_mask[j] == 0) continue;      // Apply padding mask
        sum_grad_attn += grad_attn[row * seq_len + j] * cached_attn[row * seq_len + j];
    }
    
    // Compute gradient w.r.t. scores
    for (int j = 0; j < seq_len; j++) {
        if (use_mask == 1 && j > row) {
            grad_scores[row * seq_len + j] = 0.0;  // Zero out future positions in causal mask
        } else if (padding_mask[j] == 0) {
            grad_scores[row * seq_len + j] = 0.0;  // Zero out PAD positions
        } else {
            grad_scores[row * seq_len + j] = cached_attn[row * seq_len + j] * 
                (grad_attn[row * seq_len + j] - sum_grad_attn);
        }
    }
}
