#version 430
layout(local_size_x = 32) in;

layout(std430) buffer Weight { float weight[]; };                    // [model_dim, model_dim]
layout(std430) readonly buffer GradWeight { float grad_weight[]; };  // [model_dim, model_dim]
layout(std430) buffer ADAM_M { float adam_m[]; };                    // [model_dim, model_dim]
layout(std430) buffer ADAM_V { float adam_v[]; };                    // [model_dim, model_dim]

uniform int model_dim;

uniform float learning_rate;
uniform float ADAM_beta1;
uniform float ADAM_beta2;
uniform int ADAM_timestep;

const float epsilon = 1e-8;

void main() {
    uint weight_idx = gl_GlobalInvocationID.x;  // flattened weight index
    if (weight_idx >= uint(model_dim * model_dim)) return;
    
    // Get pre-computed weight gradient from attention backward pass
    float weight_gradient = grad_weight[weight_idx];
    
    // ADAM optimizer update
    adam_m[weight_idx] = ADAM_beta1 * adam_m[weight_idx] + (1.0 - ADAM_beta1) * weight_gradient;
    adam_v[weight_idx] = ADAM_beta2 * adam_v[weight_idx] + (1.0 - ADAM_beta2) * weight_gradient * weight_gradient;

    // Bias correction for ADAM
    float m_hat = adam_m[weight_idx] / (1.0 - pow(ADAM_beta1, ADAM_timestep + 1));
    float v_hat = adam_v[weight_idx] / (1.0 - pow(ADAM_beta2, ADAM_timestep + 1));

    v_hat = max(v_hat, epsilon); // Prevent sqrt of zero or negative
    float denom = sqrt(v_hat) + epsilon;
    if (denom == 0.0) denom = epsilon;
    weight[weight_idx] -= learning_rate * m_hat / denom;

    // NaN/Inf protection
    if (isnan(weight[weight_idx]) || isinf(weight[weight_idx])) weight[weight_idx] = 0.0;
} 