#version 430

layout(local_size_x = 16, local_size_y = 1, local_size_z = 1) in;

// Embedding matrix: [vocab_size, model_dim] (column-major)
layout(std430) restrict buffer EmbeddingBuffer { float embeddings[]; };
// Gradients: [seq_len, model_dim]
layout(std430) readonly buffer GradBuffer { float grads[]; };
// Token indices:
layout(std430) readonly buffer TokenIdxBuffer { int token_indices[]; };
// ADAM moment buffers: [vocab_size, model_dim]
layout(std430) buffer AdamMBuffer { float adam_m[]; };
layout(std430) buffer AdamVBuffer { float adam_v[]; };

uniform int seq_len;
uniform int vocab_size;
uniform int model_dim;
uniform float learning_rate;
uniform float ADAM_beta1;
uniform float ADAM_beta2;
uniform int ADAM_timestep;

const float epsilon = 1e-8;

void main() {
    uint dim = gl_GlobalInvocationID.x;      // model_dim (column)
    if (dim >= model_dim) return;

    // For each token in the sequence, update its embedding
    for (int out_idx = 0; out_idx < seq_len; ++out_idx) {
        int token_idx = token_indices[out_idx];
        if (token_idx < 0 || token_idx >= vocab_size) continue;
        uint idx = token_idx * model_dim + dim;
        float grad = grads[out_idx * model_dim + dim];

        // Adam update
        adam_m[idx] = ADAM_beta1 * adam_m[idx] + (1.0 - ADAM_beta1) * grad;
        adam_v[idx] = ADAM_beta2 * adam_v[idx] + (1.0 - ADAM_beta2) * grad * grad;
        float m_hat = adam_m[idx] / (1.0 - pow(ADAM_beta1, ADAM_timestep + 1));
        float v_hat = adam_v[idx] / (1.0 - pow(ADAM_beta2, ADAM_timestep + 1));
        v_hat = max(v_hat, epsilon);
        float denom = sqrt(v_hat) + epsilon;
        if (denom == 0.0) denom = epsilon;
        embeddings[idx] -= learning_rate * m_hat / denom;
        if (isnan(embeddings[idx]) || isinf(embeddings[idx])) embeddings[idx] = 0.0;
    }
} 