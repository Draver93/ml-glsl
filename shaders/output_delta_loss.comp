#version 430

layout(local_size_x = 16, local_size_y = 16, local_size_z = 1) in;

layout(std430) restrict readonly buffer OutputBuffer { float outputs[]; };
layout(std430) restrict readonly buffer TargetBuffer { float targets[]; };
layout(std430) restrict readonly buffer PreActivationBuffer { float preactivations[]; };
layout(std430) restrict writeonly buffer DeltaBuffer { float deltas[]; };

uniform int output_size;
uniform int batch_size;
uniform int activation_type;

float derivative(float z, int type) {
    switch(type) {
        case 0: { float y = tanh(z); return 1.0 - y * y; }
        case 1: return z > 0.0 ? 1.0 : 0.0;
        case 2: return z > 0.0 ? 1.0 : 0.01;
        case 3: { float y = 1.0 / (1.0 + exp(-z)); return y * (1.0 - y); }
        case 4: return 1.0;
        default: return 1.0;
    }
}

void main() {
    uint batch_idx = gl_GlobalInvocationID.x;  // batch sample index
    uint output_neuron_idx = gl_GlobalInvocationID.y;  // output neuron index
    if (batch_idx >= batch_size || output_neuron_idx >= output_size) return;
    
    // Row-major indexing for batch processing: [batch * output_size + output_neuron]
    uint output_idx = batch_idx * output_size + output_neuron_idx;
    float output_val = outputs[output_idx];
    float target_val = targets[output_idx];
    float preactivation_val = preactivations[output_idx];
    
    // Compute gradient for output layer: dL/dz = 2 * (output - target) * activation_derivative
    float output_error = 2.0 * (output_val - target_val);
    deltas[output_idx] = output_error * derivative(preactivation_val, activation_type);
}