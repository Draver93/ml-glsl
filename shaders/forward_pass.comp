#version 430

layout(local_size_x = 16, local_size_y = 16, local_size_z = 1) in;

layout(std430) restrict readonly buffer InputBuffer { float input_data[]; };
layout(std430) restrict readonly buffer WeightBuffer { float weights[]; };
layout(std430) restrict readonly buffer BiasBuffer { float biases[]; };

layout(std430) restrict writeonly buffer OutputBuffer { float output_data[]; };
layout(std430) restrict writeonly buffer PreActivationBuffer { float preactivation_data[]; };

uniform int input_size;
uniform int output_size;
uniform int batch_size;
uniform int activation_type;

float activate(float x, int type) {
    switch(type) {
        case 0: return tanh(x); // tanh
        case 1: return max(0.0, x); // relu
        case 2: return max(0.01 * x, x); // leaky relu
        case 3: return 1.0 / (1.0 + exp(-x)); // sigmoid
        case 4: return x; // identity
        default: return x;
    }
}

void main() {
    uint batch_idx = gl_GlobalInvocationID.x;  // batch sample index
    uint output_neuron_idx = gl_GlobalInvocationID.y;  // output neuron index
    
    if (batch_idx >= batch_size || output_neuron_idx >= output_size) return;
    
    // Initialize with bias for this output neuron
    float weighted_sum = biases[output_neuron_idx];
    
    // Compute weighted sum: input @ weights + bias
    // Row-major indexing for batch processing: [batch * input_size + input_neuron]
    for (int input_neuron_idx = 0; input_neuron_idx < input_size; input_neuron_idx++) {
        uint input_idx = batch_idx * input_size + input_neuron_idx;
        uint weight_idx = input_neuron_idx * output_size + output_neuron_idx;
        weighted_sum += input_data[input_idx] * weights[weight_idx];
    }
    
    // Store results with row-major indexing: [batch * output_size + output_neuron]
    uint output_idx = batch_idx * output_size + output_neuron_idx;
    preactivation_data[output_idx] = weighted_sum;  // Store pre-activation for backprop
    output_data[output_idx] = activate(weighted_sum, activation_type);  // Store post-activation
}