#version 430

layout(local_size_x = 16, local_size_y = 16, local_size_z = 1) in;

layout(std430) restrict readonly buffer InputBuffer { float inputs[]; };
layout(std430) restrict readonly buffer DeltaBuffer { float deltas[]; };

layout(std430) restrict buffer WeightBuffer { float weights[]; };
layout(std430) restrict buffer ADAM_MBuffer { float adam_m[]; };
layout(std430) restrict buffer ADAM_VBuffer { float adam_v[]; };

uniform int input_size;
uniform int output_size;
uniform int batch_size;

uniform float learning_rate;
uniform float ADAM_beta1;
uniform float ADAM_beta2;
uniform int ADAM_timestep;
uniform int use_batch_idx = -1;

const float epsilon = 1e-8;

void main() {
    uint input_neuron_idx = gl_GlobalInvocationID.x;  // input neuron index
    uint output_neuron_idx = gl_GlobalInvocationID.y; // output neuron index
    uint weight_idx = input_neuron_idx * output_size + output_neuron_idx;

    if (input_neuron_idx >= input_size || output_neuron_idx >= output_size) return;
    
    // Compute weight gradient: dL/dw = input^T @ delta (accumulated across batch)
    float weight_gradient = 0.0;
    
    // Accumulate gradients across all batch samples
    for (int batch_idx = 0; batch_idx < batch_size; batch_idx++) {
        // Column-major indexing: [batch_idx * input_size/output_size + input_neuron_idx/output_neuron_idx]
        uint input_idx = batch_idx * input_size + input_neuron_idx;
        if(use_batch_idx != -1) input_idx = use_batch_idx * input_size + input_neuron_idx;
        
        uint delta_idx = batch_idx * output_size + output_neuron_idx;
        weight_gradient += inputs[input_idx] * deltas[delta_idx];
    }

    // Average gradient across batch
    weight_gradient /= float(batch_size);

    // ADAM optimizer update
    adam_m[weight_idx] = ADAM_beta1 * adam_m[weight_idx] + (1.0 - ADAM_beta1) * weight_gradient;
    adam_v[weight_idx] = ADAM_beta2 * adam_v[weight_idx] + (1.0 - ADAM_beta2) * weight_gradient * weight_gradient;

    // Bias correction for ADAM
    float m_hat = adam_m[weight_idx] / (1.0 - pow(ADAM_beta1, ADAM_timestep + 1));
    float v_hat = adam_v[weight_idx] / (1.0 - pow(ADAM_beta2, ADAM_timestep + 1));

    v_hat = max(v_hat, epsilon); // Prevent sqrt of zero or negative
    float denom = sqrt(v_hat) + epsilon;
    if (denom == 0.0) denom = epsilon;
    weights[weight_idx] -= learning_rate * m_hat / denom;
    
    // NaN/Inf protection
    if (isnan(weights[weight_idx]) || isinf(weights[weight_idx])) weights[weight_idx] = 0.0;
}