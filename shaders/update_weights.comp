#version 430

layout(local_size_x = 16, local_size_y = 16, local_size_z = 1) in;

layout(std430) restrict readonly buffer InputBuffer { float inputs[]; };
layout(std430) restrict readonly buffer DeltaBuffer { float deltas[]; };

layout(std430) restrict buffer WeightBuffer { float weights[]; };
layout(std430) restrict buffer ADAM_MBuffer { float adam_m[]; };
layout(std430) restrict buffer ADAM_VBuffer { float adam_v[]; };

uniform int input_size;
uniform int output_size;
uniform int batch_size;
uniform float ADAM_beta1;
uniform float ADAM_beta2;
uniform int ADAM_timestep;
uniform float learning_rate;

const float epsilon = 1e-8;

void main() {
    uint input_idx = gl_GlobalInvocationID.x;
    uint output_idx = gl_GlobalInvocationID.y;
    uint idx = input_idx * output_size + output_idx;

    if (input_idx >= input_size || output_idx >= output_size) return;
    
    float weight_gradient = 0.0;
    
    // Accumulate gradients across batch
    for (int b = 0; b < batch_size; b++) {
        float input_val = inputs[b * input_size + input_idx];
        float delta_val = deltas[b * output_size + output_idx];
        
        weight_gradient += input_val * delta_val;
    }

    // Average gradient and update weight
    weight_gradient /= float(batch_size);

    adam_m[idx] = ADAM_beta1 * adam_m[idx] + (1.0 - ADAM_beta1) * weight_gradient;
    adam_v[idx] = ADAM_beta2 * adam_v[idx] + (1.0 - ADAM_beta2) * weight_gradient * weight_gradient;

    // Bias correction (optional)
    float m_hat = adam_m[idx] / (1.0 - pow(ADAM_beta1, ADAM_timestep + 1));
    float v_hat = adam_v[idx] / (1.0 - pow(ADAM_beta2, ADAM_timestep + 1));

    weights[idx] -= learning_rate * m_hat / (sqrt(v_hat) + epsilon); //ADAM
    //weights[idx] -= learning_rate * weight_gradient; SGD
}