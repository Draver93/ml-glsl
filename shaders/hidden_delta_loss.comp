#version 430

layout(local_size_x = 16, local_size_y = 16, local_size_z = 1) in;

layout(std430) restrict readonly buffer PreActivationBuffer { float preactivations[]; };
layout(std430) restrict readonly buffer NextDeltaBuffer { float next_deltas[]; };
layout(std430) restrict readonly buffer WeightBuffer { float weights[]; };
layout(std430) restrict writeonly buffer DeltaBuffer { float deltas[]; };

uniform int current_size;
uniform int next_size;
uniform int batch_size;
uniform int activation_type;

float derivative(float z, int type) {
    switch(type) {
        case 0: { float y = tanh(z); return 1.0 - y * y; }
        case 1: return z > 0.0 ? 1.0 : 0.0;
        case 2: return z > 0.0 ? 1.0 : 0.01;
        case 3: { float y = 1.0 / (1.0 + exp(-z)); return y * (1.0 - y); }
        case 4: return 1.0;
        default: return 1.0;
    }
}

void main() {
    uint batch_idx = gl_GlobalInvocationID.x;  // batch sample index
    uint current_neuron_idx = gl_GlobalInvocationID.y;  // current layer neuron index
    if (batch_idx >= batch_size || current_neuron_idx >= current_size) return;
    
    // Compute gradient for hidden layer: dL/dz = (next_deltas @ weights^T) * activation_derivative
    float gradient_sum = 0.0;
    for (int next_neuron_idx = 0; next_neuron_idx < next_size; next_neuron_idx++) {
        // Column-major indexing for batch processing
        uint next_delta_idx = batch_idx * next_size + next_neuron_idx;
        uint weight_idx = current_neuron_idx * next_size + next_neuron_idx;
        gradient_sum += next_deltas[next_delta_idx] * weights[weight_idx];
    }
    
    // Apply activation derivative
    uint current_idx = batch_idx * current_size + current_neuron_idx;
    float preactivation = preactivations[current_idx];
    deltas[current_idx] = gradient_sum * derivative(preactivation, activation_type);
}